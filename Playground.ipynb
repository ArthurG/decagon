{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing\n",
    "\n",
    "from polypharmacy.utility import *\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "# Train on GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# Functions\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "\n",
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    edge_ind = 0\n",
    "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
    "\n",
    "        actual.append(edge_ind)\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    apk_sc = rank_metrics.apk(actual, predicted, k=50)\n",
    "\n",
    "    return roc_sc, aupr_sc, apk_sc\n",
    "\n",
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data/bio-decagon-ppi.csv\n",
      "Edges: 715612\n",
      "Nodes: 19081\n",
      "Reading: data/bio-decagon-targets.csv\n",
      "Reading: data/bio-decagon-combo.csv\n",
      "Drug combinations: 63473 Side effects: 1317\n",
      "Drug-drug interactions: 4649441\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Load and preprocess data (This is a dummy toy example!)\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "####\n",
    "# The following code uses artificially generated and very small networks.\n",
    "# Expect less than excellent performance as these random networks do not have any interesting structure.\n",
    "# The purpose of main.py is to show how to use the code!\n",
    "#\n",
    "# All preprocessed datasets used in the drug combination study are at: http://snap.stanford.edu/decagon:\n",
    "# (1) Download datasets from http://snap.stanford.edu/decagon to your local machine.\n",
    "# (2) Replace dummy toy datasets used here with the actual datasets you just downloaded.\n",
    "# (3) Train & test the model.\n",
    "####\n",
    "\n",
    "val_test_size = 0.05\n",
    "\n",
    "\n",
    "gene_net, node2idx = load_ppi_v2(\"data/bio-decagon-ppi.csv\") # protein protein interactions\n",
    "stitch2proteins = load_targets(\"data/bio-decagon-targets.csv\") #drug protein interations\n",
    "combo2stitch, combo2se, se2name = load_combo_se('data/bio-decagon-combo.csv') #Drug drug interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean drug data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_se_counter(se_map):\n",
    "    side_effects = []\n",
    "    for drug in se_map:\n",
    "        side_effects += list(set(se_map[drug]))\n",
    "    return Counter(side_effects)\n",
    "\n",
    "combo_counter = get_se_counter(combo2se)\n",
    "\n",
    "common_se = []\n",
    "for se, count in combo_counter.most_common(964):\n",
    "    common_se += [se]\n",
    "common_se = set(common_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of side effects 964\n"
     ]
    }
   ],
   "source": [
    "# Do some pre-processing of drug data\n",
    "\n",
    "#count up all the unique drugs, and give them a unique id\n",
    "drug2idx = {}\n",
    "idx = 0\n",
    "for combo, se_set in combo2se.items():     \n",
    "    if len(combo2se[combo].intersection(common_se)) == 0:\n",
    "        continue\n",
    "    drug_0 = combo.split(\"_\")[0]\n",
    "    drug_1 = combo.split(\"_\")[1]\n",
    "    if drug_0 not in drug2idx:\n",
    "        drug2idx[drug_0] = idx\n",
    "        idx+=1\n",
    "    if drug_1 not in drug2idx:\n",
    "        drug2idx[drug_1] = idx\n",
    "        idx+=1\n",
    "n_drugs = len(drug2idx)\n",
    "#count up all the unique side effects, give them a unique id\n",
    "idx = 0\n",
    "se2idx = {}\n",
    "for _, se_set in combo2se.items(): \n",
    "    for se in se_set:\n",
    "        if se not in se2idx and se in common_se:\n",
    "            se2idx[se] = idx\n",
    "            idx+=1\n",
    "        \n",
    "print(\"Number of side effects\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drug-drug interactions 4576785\n"
     ]
    }
   ],
   "source": [
    "# Need to create a drug - drug interaction matrix\n",
    "drug_drug_adj_list = [np.zeros((n_drugs, n_drugs)) for _ in se2idx]\n",
    "\n",
    "count= 0\n",
    "for drug_drug, se_set in combo2se.items():\n",
    "    drug0 = drug_drug.split(\"_\")[0]\n",
    "    drug1 = drug_drug.split(\"_\")[1]\n",
    "    for se in se_set:\n",
    "        if drug0 in drug2idx and  drug1 in drug2idx and se in se2idx:\n",
    "            se_index = se2idx[se]\n",
    "            drug0_index = drug2idx[drug0]\n",
    "            drug1_index = drug2idx[drug1]\n",
    "            drug_drug_adj_list[se_index][drug0_index][drug1_index] = 1        \n",
    "            drug_drug_adj_list[se_index][drug1_index][drug0_index] = 1\n",
    "            count+=1\n",
    "        \n",
    "drug_drug_adj_list = [ sp.csr_matrix(item) for item in drug_drug_adj_list]\n",
    "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]\n",
    "        \n",
    "print(\"Number of drug-drug interactions\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with genes (aka proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins 19081\n"
     ]
    }
   ],
   "source": [
    "gene_adj = nx.adjacency_matrix(gene_net)\n",
    "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
    "n_genes = gene_adj.shape[0]\n",
    "print(\"Number of proteins\", n_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715612.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(gene_degrees)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with drug - protein interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find 94 out of 18596 proteins (0.00505485050548505)\n",
      "Number of protein-drug edges 18596\n"
     ]
    }
   ],
   "source": [
    "missing = 0 \n",
    "present = 0\n",
    "gene_drug_matrix =  np.zeros((n_genes, n_drugs))\n",
    "for (drug_id, protein_set) in stitch2proteins.items():\n",
    "    drug_idx = drug2idx[drug_id]\n",
    "    #print(drug_id)\n",
    "    #print(drug2idx[drug_id])\n",
    "    #print(protein_set)\n",
    "    for item in protein_set:\n",
    "        if item not in node2idx:\n",
    "            missing+=1\n",
    "        else:\n",
    "            protein_idx = node2idx[item]\n",
    "        #print(node2idx[item])\n",
    "            gene_drug_matrix[protein_idx][drug_idx] = 1\n",
    "            present+=1\n",
    "    #break\n",
    "print(\"Unable to find {} out of {} proteins ({})\".format(missing, present, missing/present))\n",
    "print(\"Number of protein-drug edges\", present)\n",
    "\n",
    "gene_drug_adj = sp.csr_matrix(gene_drug_matrix)\n",
    "drug_gene_adj = gene_drug_adj.transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19081, 645)\n"
     ]
    }
   ],
   "source": [
    "print(gene_drug_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge types: 968\n"
     ]
    }
   ],
   "source": [
    "# data representation\n",
    "adj_mats_orig = {\n",
    "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
    "    (0, 1): [gene_drug_adj],\n",
    "    (1, 0): [drug_gene_adj],\n",
    "#    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list]#, should not be necesssary given how i loaded in the data\n",
    "    (1, 1): drug_drug_adj_list,\n",
    "}\n",
    "degrees = {\n",
    "    0: [gene_degrees, gene_degrees],\n",
    "#    1: drug_degrees_list + drug_degrees_list,\n",
    "    1: drug_degrees_list,\n",
    "}\n",
    "\n",
    "# featureless (genes)\n",
    "gene_feat = sp.identity(n_genes)\n",
    "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
    "gene_feat = preprocessing.sparse_to_tuple(gene_feat.tocoo())\n",
    "\n",
    "# features (drugs)\n",
    "drug_feat = sp.identity(n_drugs)\n",
    "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
    "drug_feat = preprocessing.sparse_to_tuple(drug_feat.tocoo())\n",
    "\n",
    "# data representation\n",
    "num_feat = {\n",
    "    0: gene_num_feat,\n",
    "    1: drug_num_feat,\n",
    "}\n",
    "nonzero_feat = {\n",
    "    0: gene_nonzero_feat,\n",
    "    1: drug_nonzero_feat,\n",
    "}\n",
    "feat = {\n",
    "    0: gene_feat,\n",
    "    1: drug_feat,\n",
    "}\n",
    "\n",
    "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
    "edge_type2decoder = {\n",
    "    (0, 0): 'bilinear',\n",
    "    (0, 1): 'bilinear',\n",
    "    (1, 0): 'bilinear',\n",
    "    (1, 1): 'dedicom',\n",
    "}\n",
    "\n",
    "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
    "num_edge_types = sum(edge_types.values())\n",
    "print(\"Edge types:\", \"%d\" % num_edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Settings and placeholders\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 50, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
    "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
    "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
    "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
    "# substantial amount of time\n",
    "PRINT_PROGRESS_EVERY = 150\n",
    "\n",
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create minibatch iterator\n",
      "Minibatch edge type: (0, 0, 0)\n",
      "Constructing test edges= 0000/71561\n",
      "Constructing test edges= 1000/71561\n",
      "Constructing test edges= 2000/71561\n",
      "Constructing test edges= 3000/71561\n",
      "Constructing test edges= 4000/71561\n",
      "Constructing test edges= 5000/71561\n",
      "Constructing test edges= 6000/71561\n",
      "Constructing test edges= 7000/71561\n",
      "Constructing test edges= 8000/71561\n",
      "Constructing test edges= 9000/71561\n",
      "Constructing test edges= 10000/71561\n",
      "Constructing test edges= 11000/71561\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Create minibatch iterator, model and optimizer\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "print(\"Create minibatch iterator\")\n",
    "minibatch = EdgeMinibatchIterator(\n",
    "    adj_mats=adj_mats_orig,\n",
    "    feat=feat,\n",
    "    edge_types=edge_types,\n",
    "    batch_size=512,\n",
    "    val_test_size=val_test_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "fileObject = open(\"minibatch.pickle\",'rb')  \n",
    "minibatch = pickle.dump(minibatch, fileObject)  \n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "WARNING:tensorflow:From /home/ubuntu/Programming/decagon/decagon/deep/layers.py:93: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "from decagon.deep.model import DecagonModel\n",
    "\n",
    "\n",
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "    placeholders=placeholders,\n",
    "    num_feat=num_feat,\n",
    "    nonzero_feat=nonzero_feat,\n",
    "    edge_types=edge_types,\n",
    "    decoders=edge_type2decoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = DecagonOptimizer(\n",
    "        embeddings=model.embeddings,\n",
    "        latent_inters=model.latent_inters,\n",
    "        latent_varies=model.latent_varies,\n",
    "        degrees=degrees,\n",
    "        edge_types=edge_types,\n",
    "        edge_type2dim=edge_type2dim,\n",
    "        placeholders=placeholders,\n",
    "        batch_size=512,\n",
    "        margin=0.1\n",
    "    )\n",
    "\n",
    "print(\"Initialize session\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Epoch: 0001 Iter: 0001 Edge: 0000 train_loss= 74.46896 val_roc= 0.62443 val_auprc= 0.60559 val_apk= 0.63149 time= 5.27305\n",
      "Epoch: 0001 Iter: 0151 Edge: 0003 train_loss= 3.20266 val_roc= 0.93784 val_auprc= 0.91946 val_apk= 0.93278 time= 4.40484\n",
      "Epoch: 0001 Iter: 0301 Edge: 0000 train_loss= 17.88869 val_roc= 0.86500 val_auprc= 0.85580 val_apk= 0.86297 time= 5.12227\n",
      "Epoch: 0001 Iter: 0451 Edge: 0003 train_loss= 3.58040 val_roc= 0.88914 val_auprc= 0.87440 val_apk= 1.00000 time= 4.42272\n",
      "Epoch: 0001 Iter: 0601 Edge: 0000 train_loss= 14.86153 val_roc= 0.88814 val_auprc= 0.87232 val_apk= 0.88346 time= 5.13428\n",
      "Epoch: 0001 Iter: 0751 Edge: 0003 train_loss= 2.70514 val_roc= 0.94138 val_auprc= 0.93194 val_apk= 1.00000 time= 4.38467\n",
      "Epoch: 0001 Iter: 0901 Edge: 0000 train_loss= 12.01385 val_roc= 0.89682 val_auprc= 0.88312 val_apk= 0.94717 time= 5.09669\n",
      "Epoch: 0001 Iter: 1051 Edge: 0003 train_loss= 2.66828 val_roc= 0.92740 val_auprc= 0.91610 val_apk= 1.00000 time= 4.46509\n",
      "Epoch: 0001 Iter: 1201 Edge: 0000 train_loss= 12.61640 val_roc= 0.90342 val_auprc= 0.89026 val_apk= 0.95007 time= 5.07381\n",
      "Epoch: 0001 Iter: 1351 Edge: 0003 train_loss= 2.07918 val_roc= 0.92712 val_auprc= 0.91994 val_apk= 1.00000 time= 4.40617\n",
      "Epoch: 0001 Iter: 1501 Edge: 0000 train_loss= 11.02122 val_roc= 0.91314 val_auprc= 0.90102 val_apk= 0.93323 time= 5.05327\n",
      "Epoch: 0001 Iter: 1651 Edge: 0003 train_loss= 2.96805 val_roc= 0.96800 val_auprc= 0.96586 val_apk= 1.00000 time= 4.38778\n",
      "Epoch: 0001 Iter: 1801 Edge: 0000 train_loss= 9.31272 val_roc= 0.92242 val_auprc= 0.91033 val_apk= 0.96554 time= 5.05826\n",
      "Epoch: 0001 Iter: 1951 Edge: 0003 train_loss= 3.98377 val_roc= 0.97946 val_auprc= 0.97876 val_apk= 1.00000 time= 4.42095\n",
      "Epoch: 0001 Iter: 2101 Edge: 0000 train_loss= 8.76817 val_roc= 0.92775 val_auprc= 0.91855 val_apk= 0.92107 time= 5.13405\n",
      "Epoch: 0001 Iter: 2251 Edge: 0003 train_loss= 3.50497 val_roc= 0.98943 val_auprc= 0.98870 val_apk= 1.00000 time= 4.38724\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################################\n",
    "#\n",
    "# Train model\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "print(\"Train model\")\n",
    "for epoch in range(50):\n",
    "\n",
    "    minibatch.shuffle()\n",
    "    itr = 0\n",
    "    while not minibatch.end():\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "        feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=0.1,\n",
    "            placeholders=placeholders)\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # Training step: run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
    "        train_cost = outs[1]\n",
    "        batch_edge_type = outs[2]\n",
    "\n",
    "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
    "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
    "                minibatch.val_edges, minibatch.val_edges_false,\n",
    "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
    "\n",
    "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
    "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
    "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "print(\"Optimization finished!\")\n",
    "\n",
    "for et in range(num_edge_types):\n",
    "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
    "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
    "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
